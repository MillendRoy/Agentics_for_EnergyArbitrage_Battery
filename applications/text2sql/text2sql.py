import os
from typing import Optional,Union,  List, Dict, Any
from pydantic import BaseModel, ConfigDict, Field
from dotenv import find_dotenv, load_dotenv
import time
from mcp import StdioServerParameters # For Stdio Server
import asyncio
import sys
from pathlib import Path
import random
sys.path.append(str(Path(__file__).resolve().parent))
from utils import get_schema_from_sqllite, get_schema_from_file, async_execute_sql, evaluate_execution_accuracy
from crewai.tools import tool
from agentics import AG

from crewai_tools import MCPServerAdapter
from agentics import AG

load_dotenv(find_dotenv())



class AnswerAssessment(BaseModel):
    question:Optional[str]
    sql:Optional[str]
    output_dataframe:Optional[str]
    answer_quality_score:Optional[float]=Field(None,ge=0,le=1,description="1 if the answer is correct and you are certain about, 0 if the answer if likely wrong or resulted in an error. Return a number between 0 and 1 when you are uncertain")
    answer_quality_assessment:Optional[str]=Field(None,description="Explain the rationale why you provide your grade to the answer.")


class Text2sqlQuestion(BaseModel): 
    question: Optional[str] =None
    db_id: Optional[str] =None
    benchmark_id: Optional[str] =None
    sql: Optional[Union[str,list[str]]] =None
    query: Optional[str] =None
    evidence: Optional[Union[str,list[str]]]=None
    reasoning_type: Optional[str] =None
    commonsense_knowledge: Optional[str] =None
    schema: Optional[str] = None
    alternative_sql_queries: Optional[List[str]] = Field(
        None, 
        description="""Generate alternative 5 SQL queries that have higher chances to 
        provide the correct answer to the question. Make sure those are diversified 
        to cover multiple different strategies to get the right answer""")
    alternative_answer_assessments:Optional[List[AnswerAssessment]] = []
    endpoint_id:Optional[str] =None
    generated_query: Optional[str] =Field(None, description="The query generated by AI")
    answer_assessment: Optional[AnswerAssessment] = Field(None, description="An assessment on the quality of the generated answer")
    system_output_df: Optional[str] = None
    gt_output_df: Optional[str] = None


async def get_schema_map(state:Text2sqlQuestion)-> Text2sqlQuestion:
    # schema_path = os.path.join(os.getenv("SQL_DB_PATH"), 
    #                         state.db_id,state.db_id+".sqlite" )
    state.schema = str(get_schema_from_file(state.benchmark_id, state.db_id))
    return state

## Define a Crew AI tool to get news for a given date using the DDGS search engine
@tool("execute_sql_query")
async def execute_sql_query(sql_query:str, db_id:str)-> str:
    """Execute a SQL query against the target db and return the execution results (error or json dataframe)"""
    schema_path = os.path.join(os.getenv("SQL_DB_PATH"), 
                            db_id,db_id+".sqlite" )
    system_output_df= await async_execute_sql(sql_query, schema_path)
    return system_output_df 


async def execute_query_map(state:Text2sqlQuestion)-> Text2sqlQuestion:
    schema_path=None
    if not state.endpoint_id:
        schema_path = os.path.join(os.getenv("SQL_DB_PATH"), 
                                state.benchmark_id,state.db_id, state.db_id+".sqlite" )
    
    state.system_output_df= await async_execute_sql(state.generated_query, db_path = schema_path, endpoint_id=state.endpoint_id)
    state.gt_output_df= await async_execute_sql(state.query or (state.sql[0] if type(state.sql)==list else state.sql), db_path = schema_path, endpoint_id=state.endpoint_id)
    return state

def get_training_data(training_dataset:str, n_shots=3) -> AG:
    training = AG.from_jsonl(training_dataset,jsonl=False)
    training = training.rebind_atype(Text2sqlQuestion)
    few_shots= AG(atype=Text2sqlQuestion)
    for i in range(n_shots):
        selected_question = random.choice(training.states)
        selected_question.generated_query = selected_question.sql
        few_shots.states.append(selected_question)
    return few_shots

async def execute_alternative_sql(state:Text2sqlQuestion)-> Text2sqlQuestion:
    schema_path = None
    if not state.endpoint_id:
        schema_path = os.path.join(os.getenv("SQL_DB_PATH"), 
                                state.benchmark_id,state.db_id, state.db_id+".sqlite" )
    
    answer_assessments = AG(atype=AnswerAssessment)
    for alternative_sql in state.alternative_sql_queries:
        query_execution = await async_execute_sql(alternative_sql,endpoint_id=state.endpoint_id, db_path=schema_path)
        answer_assessments.states.append(AnswerAssessment(sql=alternative_sql,
                                output_dataframe =query_execution,
                                question=state.question))
    alternative_answer_assessments = await answer_assessments.self_transduction(["question","generated_question","output_dataframe"],["answer_quality_score" , "answer_quality_assessment"])
    state.alternative_answer_assessments = alternative_answer_assessments.states
    return state

async def select_best_answer(state:Text2sqlQuestion)-> Text2sqlQuestion:
    best_selected_score = 0
    selected_best_answer=None
    for answer in state.alternative_answer_assessments + ([state.answer_assessment] if state.answer_assessment else []):
        if answer.answer_quality_score and answer.answer_quality_score > best_selected_score:
        
            selected_best_answer=answer
            best_selected_score = answer.answer_quality_score
    if selected_best_answer:
        state.generated_query=selected_best_answer.sql
    return state

async def perform_answer_validation(test:AG, n_queries:int=5)-> AG:
        
    test = await test.self_transduction(
            ["question","db_id", "schema","evidence", "commonsense_knowledge","answer_assessment"],
            ["alternative_sql_queries"], 
            instructions=f"""You previously generated , run , executed and assessed a SQL query needed to get 
            information to answer a given utterance. Your task is to formulate {n_queries} alternative SQL queries that 
            have better chances to gather the information needed to answer the given question.""")
    
    test = await test.amap(execute_alternative_sql)
    test = await test.amap(select_best_answer)
    test= await test.amap(execute_query_map)
    return test


async def execute_multiple_queries(test:AG, n_queries:int=5)-> AG:
    
    test = await test.self_transduction(
            ["question","db_id", "schema","evidence", "commonsense_knowledge"],
            ["alternative_sql_queries"], 
            instructions=f""""Your task is to convert a natural language question into an accurate SQL query using the given the database schema.\n\n"
            "**Instructions:**\n"
            "- Only use columns listed in the schema.\n"
            "- Do not use any other columns or tables not mentioned in the schema.\n"
            "- Ensure the SQL query is valid and executable.\n"
            "- Use proper SQL syntax and conventions.\n"
            "- Generate a complete SQL query that answers the question.\n"
            "- Use the correct SQL dialect for SQLite \n"
            "- Do not include any explanations or comments in the SQL output.\n"
            "- Generate 5 alternative sql queries that you will later execute and validate to pick the best one. 
    )""")
    
    test = await test.amap(execute_alternative_sql)
    test = await test.amap(select_best_answer)
    test= await test.amap(execute_query_map)
    return test


async def execute_questions(test:AG, 
                            few_shots_path:str = None,
                            answer_validation: bool = True):
    begin_time=time.time()
    training = AG(atype=Text2sqlQuestion)
    if few_shots_path:
        training = get_training_data(few_shots_path)
    test.llm=AG.get_llm_provider("watsonx")
    test.reasoning=False
    # test.tools=[execute_sql_query]
    # test.max_iter=10
    ## add training data
    test.states= training.states+test.states
    test= await test.amap(get_schema_map)
    
    test.verbose_agent=False

    test = await baseline_zero_shot(test)

    if answer_validation == True:
        test = await perform_answer_validation(test)

    
    
    print(f"task executed in {time.time() - begin_time} seconds")
    test.states=test.states[len(training.states):]
    return test




async def baseline_zero_shot(test:AG)-> AG:

    test = await test.self_transduction(
        ["question","db_id", "schema","evidence","commonsense_knowledge"], 
        ["generated_query"], 
        instructions=
            "Your task is to convert a natural language question into an accurate SQL query using the given the database schema.\n\n"
            "**Instructions:**\n"
            "- Only use columns listed in the schema.\n"
            "- Do not use any other columns or tables not mentioned in the schema.\n"
            "- Ensure the SQL query is valid and executable.\n"
            "- Use proper SQL syntax and conventions.\n"
            "- Generate a complete SQL query that answers the question.\n"
            "- Use the correct SQL dialect for SQLite \n"
            "- Do not include any explanations or comments in the SQL output.\n"
    )
    test= await test.amap(execute_query_map)
    return test


async def run_evaluation_benchmark(benchmark_id = "archer_en_dev",
                       max_rows: int = None,
                       few_shots_path: str = None):
    
    benchmarks= os.listdir(os.getenv("SQL_BENCHMARKS_FOLDER"))
    if benchmark_id + ".json" in benchmarks:
        test = AG.from_jsonl(
                    os.path.join(os.getenv("SQL_BENCHMARKS_FOLDER"), 
                                          benchmark_id + ".json"),     
                    jsonl=False,
                    atype=Text2sqlQuestion,
                    max_rows=max_rows)
       
        new_states=[]
        for state in test:
            state.benchmark_id = benchmark_id
            new_states.append(state)
        test.states=new_states

        test = await execute_questions(test, few_shots_path=few_shots_path, benchmark_id="archer_en_dev")
        print(evaluate_execution_accuracy(test))
        return test
        

# test = AG.from_jsonl("/Users/gliozzo/Data/Text2SQL/Experiments/bird_mini_dev_sqlite_baseline.jsonl", atype=Text2sqlQuestion)
# evaluate_execution_accuracy(test)
