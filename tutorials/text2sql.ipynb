{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! uv pip install agentics-py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import time\n",
    "\n",
    "CURRENT_PATH = \"\"\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "print(\"In Colab:\", IN_COLAB)\n",
    "\n",
    "\n",
    "if IN_COLAB:\n",
    "    CURRENT_PATH = \"/content/drive/MyDrive/\"\n",
    "    # Mount your google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    load_dotenv(\"/content/drive/MyDrive/.env\")\n",
    "else:\n",
    "    load_dotenv(find_dotenv())\n",
    "\n",
    "if not os.getenv(\"GEMINI_API_KEY\"):\n",
    "    os.environ[\"GEMINI_API_KEY\"] = getpass(\"Enter your GEMINI_API_KEY:\")\n",
    "\n",
    "begin_time=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, List, Dict, Any\n",
    "from pydantic import BaseModel, ConfigDict, Field\n",
    "from pandas import DataFrame\n",
    "\n",
    "class Text2sqlQuestion(BaseModel): \n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "    question: Optional[str] =None\n",
    "    db_id: Optional[str] =None\n",
    "    query: Optional[str] =None\n",
    "    reasoning_type: Optional[str] =None\n",
    "    commonsense_knowledge: Optional[str] =None\n",
    "    schema: Optional[str] = None\n",
    "    generated_query: Optional[str] =Field(None, description=\"The query generated by AI\")\n",
    "    system_output_df: Optional[str] = None\n",
    "    gt_output_df: Optional[str] = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def fix_double_quoted_literals(sql: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert double-quoted *literals* to single-quoted strings.\n",
    "    Keep double-quoted *identifiers* like \"MyTable\" as-is.\n",
    "\n",
    "    Heuristic: if the content is a simple identifier ([A-Za-z_][A-Za-z0-9_]*),\n",
    "    we keep the double quotes; otherwise we treat it as a literal and convert.\n",
    "    \"\"\"\n",
    "    ident_re = re.compile(r'^[A-Za-z_][A-Za-z0-9_]*$')\n",
    "\n",
    "    def repl(m):\n",
    "        body = m.group(1).replace('\"\"', '\"')  # unescape doubled quotes inside \"\"\n",
    "        if ident_re.fullmatch(body):\n",
    "            # looks like an identifier → leave as \"Identifier\"\n",
    "            return f'\"{m.group(1)}\"'\n",
    "        # looks like a literal → convert to '...'\n",
    "        return \"'\" + body.replace(\"'\", \"''\") + \"'\"\n",
    "\n",
    "    # Match \" ... \" allowing doubled \"\" inside\n",
    "    return re.sub(r'\"((?:[^\"]|\"\")*)\"', repl, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import aiosqlite\n",
    "import asyncio\n",
    "\n",
    "async def async_execute_sql(sql_query: str, db_path:str) -> str:\n",
    "        try:\n",
    "            async with aiosqlite.connect(db_path) as db:\n",
    "                async with db.execute(sql_query.replace(\"\\\"\",\"'\")) as cursor:\n",
    "                    columns = [description[0] for description in cursor.description]\n",
    "                    rows = await asyncio.wait_for(cursor.fetchall(), timeout=10)\n",
    "                    df = DataFrame(rows, columns=columns)\n",
    "                    return df.to_json()\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "def get_schema(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "    schema_json={}\n",
    "    for table in tables:\n",
    "\n",
    "        cursor.execute(f\"PRAGMA table_info({table[0]});\")\n",
    "        schema = cursor.fetchall()\n",
    "        \n",
    "        schema_json[table[0]] = { col[1] : {\"type\" : col[2], \"notnull\" : col[3], \"dflt_value\": col[4], } for col in schema} \n",
    "    return schema_json\n",
    "\n",
    "res = get_schema(\"/Users/gliozzo/Data/Text2SQL/Archer/database/bike_1/bike_1.sqlite\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Set\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def remove_duplicate_col_df(df):\n",
    "    return df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "\n",
    "def convert_df_to_set(df, row_invariant=True) -> Set:\n",
    "    # remove duplicate columns\n",
    "    df = remove_duplicate_col_df(df)\n",
    "\n",
    "    if row_invariant:\n",
    "        return set(\n",
    "            [\n",
    "                tuple(sorted(df[c].to_list(), key=lambda x: (x is None, str(x))))\n",
    "                for c in df.columns.values\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        return set([tuple(df[c].to_list()) for c in df.columns.values])\n",
    "\n",
    "\n",
    "\n",
    "def compare_df(gt, predicted, row_invariant=False) -> bool:\n",
    "    # 1: gt_df is subset of predicted_df\n",
    "    # 2: df1 == df2\n",
    "    # 0: otherwise\n",
    "    if predicted.startswith(\"Error:\") or gt.startswith(\"Error:\"): return 0\n",
    "    gt_df = pd.read_json(gt)\n",
    "    predicted_df = pd.read_json(predicted)\n",
    "    gt_df = gt_df.map(lambda x: float(f\"{x:.5f}\") if isinstance(x, float) else x)\n",
    "    predicted_df = predicted_df.map(\n",
    "        lambda x: float(f\"{x:.5f}\") if isinstance(x, float) else x\n",
    "    )\n",
    "\n",
    "    gt_set = convert_df_to_set(gt_df, row_invariant=row_invariant)\n",
    "    predicted_set = convert_df_to_set(predicted_df, row_invariant=row_invariant)\n",
    "\n",
    "    intersec = gt_set & predicted_set\n",
    "    return (\n",
    "        1\n",
    "        if (intersec == gt_set)\n",
    "        else 1\n",
    "        if (predicted_set == gt_set)\n",
    "        else 1\n",
    "        if (intersec == predicted_set)\n",
    "        else 0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.tools import tool\n",
    "## Define a Crew AI tool to get news for a given date using the DDGS search engine\n",
    "@tool(\"execute_sql_query\")\n",
    "async def execute_sql_query(sql_query:str, db_id:str)-> str:\n",
    "    \"\"\"Execute a SQL query against the target db and return the execution results (error or json dataframe)\"\"\"\n",
    "    schema_path = os.path.join(\"/Users/gliozzo/Data/Text2SQL/Archer/database/\", \n",
    "                            db_id,db_id+\".sqlite\" )\n",
    "    system_output_df= await async_execute_sql(sql_query, schema_path)\n",
    "    return system_output_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentics import Agentics as AG\n",
    "from agentics.core.llm_connections import get_llm_provider\n",
    "\n",
    "training = AG.from_jsonl(\"/Users/gliozzo/Data/Text2SQL/Archer/en_data/dev.json\", \n",
    "                        jsonl=False, \n",
    "                        atype=Text2sqlQuestion,\n",
    "                        #max_rows=1,\n",
    "                        )\n",
    "training.llm=get_llm_provider(\"watsonx\")\n",
    "training.reasoning=False\n",
    "training.tools=[execute_sql_query]\n",
    "training.max_iter=10\n",
    "training[0].model_dump_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_schema_map(state:Text2sqlQuestion)-> Text2sqlQuestion:\n",
    "    schema_path = os.path.join(\"/Users/gliozzo/Data/Text2SQL/Archer/database/\", \n",
    "                            state.db_id,state.db_id+\".sqlite\" )\n",
    "    state.schema=str(get_schema(schema_path))\n",
    "    print(state.db_id)\n",
    "    return state\n",
    "\n",
    "training= await training.amap(get_schema_map)\n",
    "training.verbose_agent=False\n",
    "print(training[0].schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = await training.self_transduction([\"question\",\"schema\",\"commonsense_knowledge\", \"db_id\"], \n",
    "                            [\"generated_query\"], \n",
    "                            instructions=\"Generate a SQL query from the input question and target db schema\")\n",
    "print(training[0].generated_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_query_map(state:Text2sqlQuestion)-> Text2sqlQuestion:\n",
    "    schema_path = os.path.join(\"/Users/gliozzo/Data/Text2SQL/Archer/database/\", \n",
    "                            state.db_id,state.db_id+\".sqlite\" )\n",
    "    state.system_output_df= await async_execute_sql(state.generated_query, schema_path)\n",
    "    state.gt_output_df= await async_execute_sql(state.query, schema_path)\n",
    "    return state\n",
    "\n",
    "training= await training.amap(execute_query_map)\n",
    "print(training[0].gt_output_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "training.to_jsonl(\"/tmp/anker_task.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"task executed in {time.time() - begin_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = AG.from_jsonl(\"/tmp/anker_task.jsonl\")\n",
    "total = 0\n",
    "for question in training:\n",
    "    total+= compare_df(question.system_output_df, question.gt_output_df)\n",
    "    print(question.gt_output_df, question.system_output_df, question.generated_query,compare_df(question.system_output_df, question.gt_output_df) )\n",
    "print(f\"Test size: {len(training.states)}\\nExecution Accuracy: {total/len(training.states)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = AG.from_jsonl(\"/Users/gliozzo/Data/Text2SQL/Archer/en_data/train.json\",jsonl=False)\n",
    "reasoning_types={}\n",
    "import random\n",
    "for question in training:\n",
    "    for reasoning_type in question.reasoning_type.split(\" \"):\n",
    "        if not reasoning_type in reasoning_types: \n",
    "            reasoning_types[reasoning_type] = []\n",
    "        reasoning_types[reasoning_type].append(question)\n",
    "for reasoning_type in reasoning_types:\n",
    "    print(random.choice(reasoning_types[reasoning_type]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
