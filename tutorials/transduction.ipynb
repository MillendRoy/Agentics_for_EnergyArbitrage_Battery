{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "08fccf46"
   },
   "source": [
    "# Logical Transduction\n",
    "\n",
    "Agentics objects are capable of performing logical transduction between their states by using the provied LLMs. The logical transduction operator << can be applied between any two agentics of any type.\n",
    "\n",
    "Transduction is done between a source and a target AG when connected by the << operator.\n",
    "\n",
    "The follwing example transduces a Question into an Answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3284,
     "status": "ok",
     "timestamp": 1757074891021,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "069407b8",
    "outputId": "df336c0c-62bb-40f9-9132-2bb38f6c0c2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Colab: False\n",
      "In Colab: False\n"
     ]
    }
   ],
   "source": [
    "# ! uv pip install agentics-py\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "CURRENT_PATH = \"\"\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "print(\"In Colab:\", IN_COLAB)\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "print(\"In Colab:\", IN_COLAB)\n",
    "\n",
    "if IN_COLAB:\n",
    "    CURRENT_PATH = \"/content/drive/MyDrive/\"\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "    load_dotenv(\"/content/drive/MyDrive/.env\")\n",
    "else:\n",
    "    load_dotenv(find_dotenv())\n",
    "\n",
    "if not os.getenv(\"GEMINI_API_KEY\"):\n",
    "    os.environ[\"GEMINI_API_KEY\"] = input(\"Enter your GEMINI_API_KEY:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "333bba1a5012475a946a9d10ae2525fc",
      "958824c60e6a422cbe77d7dc95cbed16"
     ]
    },
    "executionInfo": {
     "elapsed": 14872,
     "status": "ok",
     "timestamp": 1757074905896,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "7b080d14",
    "outputId": "5d7382b0-6357-4a97-f7cd-d362152e0fd1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-04 13:27:55.244\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36magentics.core.llm_connections\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m121\u001b[0m - \u001b[34m\u001b[1mAGENTICS is connecting to the following LLM API providers:\u001b[0m\n",
      "\u001b[32m2025-10-04 13:27:55.244\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36magentics.core.llm_connections\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m129\u001b[0m - \u001b[34m\u001b[1m0 - Gemini\u001b[0m\n",
      "\u001b[32m2025-10-04 13:27:55.245\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36magentics.core.llm_connections\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m135\u001b[0m - \u001b[34m\u001b[1mPlease add API keys in .env file to add or disconnect providers.\u001b[0m\n",
      "\u001b[32m2025-10-04 13:27:55.252\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36magentics.core.llm_connections\u001b[0m:\u001b[36mget_llm_provider\u001b[0m:\u001b[36m29\u001b[0m - \u001b[34m\u001b[1mNo LLM provider specified. Using the first available provider.\u001b[0m\n",
      "\u001b[32m2025-10-04 13:27:55.252\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36magentics.core.llm_connections\u001b[0m:\u001b[36mget_llm_provider\u001b[0m:\u001b[36m31\u001b[0m - \u001b[34m\u001b[1mAvailable LLM providers: ['gemini']. Using 'gemini'\u001b[0m\n",
      "\u001b[32m2025-10-04 13:27:55.254\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36magentics.core.llm_connections\u001b[0m:\u001b[36mget_llm_provider\u001b[0m:\u001b[36m42\u001b[0m - \u001b[34m\u001b[1mUsing specified LLM provider: gemini\u001b[0m\n",
      "\u001b[32m2025-10-04 13:27:55.254\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36magentics.core.agentics\u001b[0m:\u001b[36m__lshift__\u001b[0m:\u001b[36m518\u001b[0m - \u001b[34m\u001b[1mExecuting task: Generate an object of the specified type from the following input.\n",
      "1 states will be transduced\u001b[0m\n",
      "\u001b[32m2025-10-04 13:27:55.254\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36magentics.core.agentics\u001b[0m:\u001b[36m__lshift__\u001b[0m:\u001b[36m612\u001b[0m - \u001b[34m\u001b[1mtransducer class: <class 'agentics.abstractions.pydantic_transducer.PydanticTransducerCrewAI'>\u001b[0m\n",
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTask Executor\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "Your task is to transduce a source Pydantic Object into the specified Output type. Generate only slots that are logically deduced from the input information, otherwise live then null.\n",
      "\n",
      "Read carefully the following instructions for executing your task:\n",
      "Generate an object of the specified type from the following input. SOURCE:\n",
      "{\"question\": \"What is the capital of Italy?\"}\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-04 13:27:56.033\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36magentics.core.agentics\u001b[0m:\u001b[36m__lshift__\u001b[0m:\u001b[36m648\u001b[0m - \u001b[34m\u001b[1mProcessed 1 states in 0.7782928943634033 seconds\u001b[0m\n",
      "\u001b[32m2025-10-04 13:27:56.033\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36magentics.core.agentics\u001b[0m:\u001b[36m__lshift__\u001b[0m:\u001b[36m700\u001b[0m - \u001b[34m\u001b[1m1 states processed in 0.03891464471817017 seconds average per state ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTask Executor\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "{\"answer\": \"Rome\", \"justification\": \"Rome is the capital city of Italy and has been for a long time.\", \"confidence\": 0.99}\u001b[00m\n",
      "\n",
      "\n",
      "Atype : <class '__main__.Answer'>\n",
      "answer: Rome\n",
      "justification: Rome is the capital city of Italy and has been for a long time.\n",
      "confidence: 0.99\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from agentics import Agentics as AG\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "from agentics.core.llm_connections import get_llm_provider\n",
    "\n",
    "## Define target and source types\n",
    "class Answer(BaseModel):\n",
    "    answer: Optional[str] = None\n",
    "    justification: Optional[str] = None\n",
    "    confidence: Optional[float] = None\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: Optional[str] = None\n",
    "\n",
    "\n",
    "## Instantiate the source AG with a question\n",
    "source = AG(\n",
    "    atype=Question,\n",
    "    llm=get_llm_provider(\"gemini\"),  ## You can choose between \"openai\" (i.e. get_llm_provider(\"openai\")), \"watsonx\", \"gemini\", \"vllm_crewai\"\n",
    "    ## set verbose to true to see the internal agents log. This is optional.\n",
    "    states=[Question(question=\"What is the capital of Italy?\")],\n",
    ")\n",
    "\n",
    "## Instantiate the target AG with a target type. No instances are needed for zero shot transduction\n",
    "target = AG(\n",
    "    atype=Answer,  ## You can choose between \"openai\", \"watsonx\", \"gemini\", \"vllm_crewai\"\n",
    "    verbose_agent=True,\n",
    ")  ## set verbose to true to see the internal agents log\n",
    "\n",
    "# Execute logical transduction by using the << operator between source and target AG\n",
    "answer = await (\n",
    "    target << source\n",
    ")  ## Note that << operator is asyncronus and the results should be awaited\n",
    "\n",
    "# Print the results of the transduction\n",
    "print(\n",
    "    answer.pretty_print()\n",
    ")  ## Note that confidence is a float number, while other fields are strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "b8681fea"
   },
   "source": [
    "In agentics, lists of strings can be used as sources instead of AG. Those are provided as input for the transduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1595,
     "status": "ok",
     "timestamp": 1757074907494,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "66f8d2e0",
    "outputId": "a04db6b5-016b-4c11-d374-f5f9e62dc731"
   },
   "outputs": [],
   "source": [
    "answer = await (AG(atype=Answer) << [\"Where is Paris?\"])\n",
    "answer.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "id": "2ade51f3"
   },
   "source": [
    "## Asyncronous Transduction\n",
    "\n",
    "When the source AG has more than one state, the << operator perform asyncronous transfuction for each state to the target type. Transductions are executed in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8082,
     "status": "ok",
     "timestamp": 1757074915577,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "b4ec8966",
    "outputId": "da8b8411-2788-4a5e-d616-f83e41cee8d2"
   },
   "outputs": [],
   "source": [
    "target = AG(\n",
    "    atype=Answer,\n",
    "    verbose_transduction=True,  # Set to verbose to see transduction timings and other logs\n",
    "    transduction_logs_path=\"/tmp/answers.jsonl\",\n",
    ")  # Optionally write longs of transductions on the specified path\n",
    "questions = [\n",
    "    \"Where is Paris?\",\n",
    "    \"Who is Alberto Sordi\",\n",
    "    \"When will climate change be irreverible?\",\n",
    "    \"Who is the best Jeopardy player?\",\n",
    "]\n",
    "\n",
    "answers = await (target << questions)\n",
    "\n",
    "answer.pretty_print()\n",
    "\n",
    "answers = await (target << questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "298be399"
   },
   "source": [
    "## Self Transduction\n",
    "\n",
    "Self transduction is a method of AGs that enables async execution of transductions between slots of the same object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38722,
     "status": "ok",
     "timestamp": 1757075164819,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "974143b6",
    "outputId": "0f63dbf4-462d-4be7-8ed5-e6ca62af16aa"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "## Define the Pydantic type\n",
    "class Movie(BaseModel):\n",
    "    movie_name: Optional[str] = (\n",
    "        None  ## Note that fields name should match the column name in the input csv\n",
    "    )\n",
    "    genre: Optional[str] = None\n",
    "    description: Optional[str] = None\n",
    "    tweet: Optional[str] = Field(\n",
    "        None, description=\"Generate a Tweet to advertise the movie\"\n",
    "    )\n",
    "\n",
    "\n",
    "base = Path(CURRENT_PATH)\n",
    "movies = AG.from_csv(\n",
    "    base / \"data/movies.csv\", atype=Movie, max_rows=20\n",
    ")  ## Load the input data from a csv file\n",
    "movies.verbose_transduction = True\n",
    "movies.llm = AG.get_llm_provider(\n",
    "    \"watsonx\"\n",
    ")  ## You can choose between \"openai\", \"watsonx\", \"gemini\", \"vllm_crewai\"\n",
    "\n",
    "movies_with_tweets = await movies.self_transduction(\n",
    "    [\"movie_name\", \"genre\", \"description\"],  ## source fields\n",
    "    [\"tweet\"],  ## target fields\n",
    "    ## Note that instruction are only needed when the  relation between source and target type\n",
    "    # is not innediately clear and need to be further specified or disambiguated.\n",
    "    instructions=\"Generate a tweet to advertise the release of the input movie\",\n",
    ")\n",
    "\n",
    "movies_with_tweets.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "fc007db0"
   },
   "source": [
    "As an alternative, self transduction can be encoded using logical transduction algebra which uses the AG() notation to rebind the original data into AGs of the requested subtypes. Learn more about atype manipulation in agentics [here](link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb1b7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38963,
     "status": "ok",
     "timestamp": 1757075203780,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "8dbb1b7f",
    "outputId": "d9c0de9a-56ba-498e-f0e9-051b5c78cec9"
   },
   "outputs": [],
   "source": [
    "movies = AG.from_csv(base / \"data/movies.csv\", atype=Movie, max_rows=20)\n",
    "tweets = await (\n",
    "    AG(\n",
    "        atype=movies(\"tweet\").atype,\n",
    "        instructions=\"Generate a tweet to advertise the release of the input movie\",\n",
    "    )\n",
    "    << movies(\"movie_name\", \"genre\", \"description\")\n",
    ")\n",
    "print(\n",
    "    tweets.pretty_print()\n",
    ")  ## Note that differently from self transduction the output tweets\n",
    "##has only the tweet field, whereas self transduction preserves\n",
    "## the original source data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "c3044a80"
   },
   "source": [
    "## Few Shots Transduction\n",
    "\n",
    "Few shots examples can be provided for transduction by adding instances of the target instances in correspondance to their sources . Those will be used by the LLM to infer by analogy all the Null instances of the target type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2f7a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = AG.from_csv(base / \"data/movies.csv\", max_rows=20, atype=Movie)\n",
    "\n",
    "print(movies.atype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7671,
     "status": "ok",
     "timestamp": 1757075243259,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "a4803bba",
    "outputId": "ca59e4dc-a35e-4aec-c823-37ab94ef94b4"
   },
   "outputs": [],
   "source": [
    "movies = AG.from_csv(base / \"data/movies.csv\", max_rows=20)\n",
    "## Note that obly the first 9 movies have categories\n",
    "for i, movie in enumerate(movies):\n",
    "    print(f\"{i}: {movie.genre}\")\n",
    "## predicting new genre from given examples\n",
    "all_genres = await (movies(\"genre\") << movies(\"movie_name\", \"description\"))\n",
    "print(all_genres.pretty_print())\n",
    "## few shots examples can also be used in self transfuction\n",
    "movies_with_genre = await movies.self_transduction(\n",
    "    [\"movie_name\", \"description\"], [\"genre\"]\n",
    ")\n",
    "print(movies_with_genre.pretty_print())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/IBM/Agentics/blob/Colab/tutorials/transduction.ipynb",
     "timestamp": 1757075262425
    }
   ]
  },
  "kernelspec": {
   "display_name": "agentics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
